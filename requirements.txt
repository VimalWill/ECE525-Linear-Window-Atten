datasets==4.0.0
litgpt==0.5.10
pytorch_lightning==2.5.4
torch==2.3.0+cu118
transformers==4.56.0

# Benchmarking dependencies
lm-eval>=0.4.0  # LM Evaluation Harness
numpy>=1.24.0
scipy>=1.10.0

# Optional: For advanced metrics
rouge-score>=0.1.2
sacrebleu>=2.3.0

# Visualization (already included via matplotlib in Llama_analysis.py)
matplotlib>=3.7.0
seaborn>=0.12.0
